## Norm Penalty

가장 많이 사용하는 정규화(regularization) 기법 중 하나는, 손실 함수에 가중치(weight)의 norm이 너무 커지지 않도록 하는 항을 더해주는 것입니다. 대표적으로 l1, l2 norm이 사용됩니다. 즉, 훈련을 할 때 원래의 손실 값을 줄이는 방향으로 움직일 뿐만 아니라 가중치의 크기를 줄이는 방향으로도 훈련하는 것입니다.

딥러닝에서 정규화는 특히 큰 의미를 갖는데, 깊은 모델일 수록 표현력이 너무 좋기 때문에 쉽게 과적합(overfitting)으로 다가갈 수 있습니다. 이를 방지하기 위해 일부러 표현력에 제한을 두는 것이 정규화 기법들이라고 볼 수 있습니다. 이 중에서도 가중치의 크기와 관련된 norm 패널티들은 인공신경망이 표현 가능한 범위(representation space)를 변수를 제약하는 것으로 좁히고자 합니다.

**L2 norm**

l2 norm 은 가중치 행렬(텐서)의 모든 항들의 제곱값의 합입니다.
> l2 norm은 ridge regession이라고 하기도 합니다.

$$
L_2 = \sum_i w_i^2 = ||W||_2^2
$$

이 때 손실 함수는 다음과 같이 바뀝니다. \\(\alpha\\)는 가중치가 얼마나 영향을 줄 지를 결정합니다. 0.0001 정도를 사용하지만 가중치가 너무 많으면 더 낮춰도 괜찮습니다. 가중치가 아주 많은 모델의 경우 \\(\alpha\\)로 아주 작은 값을 사용합니다. 전부 연결된 층 (fully connected layer)와 같이 가중치가 아주 많이 들어 있는 층만 l2 norm을 적용하기도 합니다.
$$
Loss_{new} = Loss_{old} + \alpha \sum_i w_i^2
$$

기울기 하강법으로 훈련된다고 생각하고, 가중치의 손실에 대한 기울기를 구해 보겠습니다. \\(\epsilon\\)은 학습 속도(learning rate)입니다.
$$
\frac{\partial Loss_{new}}{\partial w_i} = \frac{\partial Loss_{old}}{\partial w_i} + 2\alpha w_i \\
w_i' = w_i - \epsilon(2\alpha w_i + \frac{\partial Loss_{old}}{\partial w_i}) \\
w_i' = (1 - 2\epsilon\alpha) w_i - \epsilon\frac{\partial Loss_{old}}{\partial w_i}
$$

위 식에서, 기존 가중치에서 약간(\\(2\epsilon\alpha\\)) 깎인 값에서 다시 기울기를 뺀 값으로 업데이트되는 것을 확인할 수 있습니다. 이런 특징 때문에 l2 norm을 사용하는 정규화 기법을 가중치 감퇴(**weight decay**)라고 하기도 합니다.

**L1 norm**

l1 norm은 가중치 행렬의 모든 항들의 절댓값의 합입니다.
> l1 norm은 lasso regression이라고 하기도 합니다.

$$
L_1 = \sum_i |w_i| = ||W||_1
$$

이 경우 손실 함수와 기울기는 다음과 같이 바뀝니다.
$$
Loss_{new} = Loss_{old} + \alpha \sum_i |w_i| \\
\frac{\partial Loss_{new}}{\partial w_i} = \frac{\partial Loss_{old}}{\partial w_i} + \alpha \times sign(w_i)
$$

즉, 가중치 감퇴가 가중치 크기에 비례해서 일어나지 않고 언제나 상수값으로 발생합니다. 가중치가 양수였으면 음수인 방향으로, 음수였으면 양수인 방향으로 크게 움직이게 되기 때문에 l2보다 좀 더 크게크게 움직이게 됩니다. l2 norm이 값들을 0에 가까운 값에서 계속 바뀌게 한다면, l1 norm은 아예 값들을 0으로 만들어 버리는 데 유용합니다. 그래서 l1 norm을 쓰는 경우 인공신경망의 가중치 행렬이 듬성듬성(sparse)해 집니다. 보통은 l2 norm만 사용합니다.

**Elastic norm**

Elastic norm은 l1 norm과 l2 norm을 단순히 더한 값을 정규화 항으로 삼는 것입니다. 인공신경망에서는 잘 사용되지 않지만, 최적화 관련 연구에서 주로 등장하는 것 같습니다. Elastic norm을 제안한 첫 논문에서는 이 loss를 사용하면 가중치들의 값이 좀 더 뭉치는 경향이 나타난다고 합니다.
$$
Loss_{new} = Loss_{old} + \alpha \sum_i w_i^2  + \beta \sum_i |w_i|
$$

**Max norm**

Max norm은 norm의 최대값이 어떤 값 이상이 되지 못하도록 합니다. 각각의 가중치들의 절대값이 기준값 \\(c\\)보다 작은 값을 갖도록 잘라내는(clipping) 경우 l1-style max norm이 됩니다. 한편, 입력 벡터에 대한 가중치 벡터의 norm이 기준값 \\(c\\)보다 작은 값을 갖도록 하는 경우 l2-style max norm이 되고, 보통은 이걸 max norm 이라고 합니다. 이는 곱해지는 가중치 벡터의 크기를 제한해 반지름 \\(c\\)인 hypersphere 에 가두는 것으로 표현 범위를 제한합니다.

l1-style 의 경우 구현이 쉽고, l2-style 의 경우 좀 더 값들 사이의 상관관계를 잘 보존하게 됩니다. 아래 식은 l2-style max norm 입니다. 
$$
w' = w \times \min(1, \frac{c}{||w||_2^2})
$$

사실 max norm 은 추가적인 정규화를 위한 손실 항으로도 많이 사용되지만, 기울기의 범위를 제약하는 데 훨씬 많이 사용됩니다. 즉, SGD 알고리즘으로 훈련할 때 원래 가중치에 더해질 기울기 크기를 max norm 으로 잘라내거나 축소하고 더하는 방식입니다. 뒤에서 다룰 RNN같은 경우, l1 style max norm 으로 기울기 값에 제약을 두어 갑자기 가중치가 너무 많이 변하지 않도록 하는 기법을 기울기 제한(**gradient clipping**)이라고 합니다. 

---