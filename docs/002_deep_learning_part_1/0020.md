# 1050 Deep Learning Basics
#### Kyuhong Shim 2016
----

## What is Deep Learning?

Deep Learning(딥러닝)은 Neural Network(인공신경망, 이하 NN)과 떼 놓고 생각할 수 없습니다. 딥러닝은  인공신경망을 아주 깊게 연결하고 쌓아올린 구조를 훈련시켜 원하는 동작을 하도록 합니다. 인간의 신경망 구조에서 영감을 받아 만들어진 NN은 실제 신경과는 그리 닮아 있지 않지만, 몇 가지 핵심 아이디어를 공유하고 있습니다.

인간은 세상을 인식할 때 구체적인 상을 받아 머릿속에서 추상적인 상으로 바꿉니다. 영화 인사이드 아웃을 봤다면 뇌의 추상화 과정을 보셨을 것입니다. 실제로도 오랫동안 연구자들은 이와 같은 구조를 통해 뇌의 동작을 모방하고자 했습니다. 각 과정이 분리되어 있듯이, 각 동작을 하는 층이 따로 있는 구조를 만들고 이를 연결해 여러 층을 쌓아 보자고 하는 시도가 딥러닝입니다.

> 순서대로 Non-objective fragmentation, Deconstruction, Two-Dimensionalization, Non-Figurativity

![](http://i.imgur.com/TzqcYXN.png)
![](http://i.imgur.com/KHh8sry.png)
![](http://i.imgur.com/uSnkBji.png)
![](http://i.imgur.com/7CGB4ud.png)

그렇다고 딥러닝이 기존의 머신러닝 기법보다 언제나 좋은 성능을 내는 것은 아닙니다. 아무래도 이런 접근법이 잘 통하는 문제도 있지만, 그렇지 않은 문제도 있을 것입니다. 무엇보다 훈련에 어마어마한 데이터와 시간이 필요하고, 연산량이 높기 때문에 GPU같은 가속기를 사용하지 않으면 결과를 보기가 너무 힘듭니다. 그럼에도 불구하고, 기존의 머신러닝으론 손도 못 댄 문제들을 딥러닝은 척척 풀어내고 있습니다.

![](http://i.imgur.com/4JcBEHO.png)

성원용 교수님께서는 이렇게 말씀하십니다.
> 인간이 잘 할 수 있는 걸 딥러닝도 잘 할 수 있고, 인간이 잘 못하는 건 딥러닝도 잘 못할 것이다.

## Representation Learning

우리가 풀고 싶은 머신러닝 문제들은 데이터들의 공통적인 특성에 의존하고 있습니다. 그래서 우리는, 데이터를 그냥 집어넣기보다 몇 가지 과정을 거쳐 representation(표현)으로 바꿉니다. 이렇게 표현형으로 바꾼 데이터는 feature(특징)으로 만들어집니다. 기존의 머신러닝 기법들은 사람이 직접 특징을 만들어 기계에 넣어 주었지만, 사람이 생각해 낼 수 있는 특징들은 사람의 뇌가 만드는 특징들보다 훨씬 못했습니다. 
- 특징이 어떻게 목표로 이어지는지 학습하는 과정이 머신러닝입니다.
- 데이터가 기계에게 어떻게 표현되는지까지도 학습하는 과정이 딥러닝입니다.
- 이 두 과정은 따로 이루어지는 것이 아니라 같이 이루어집니다.

딥러닝에서 표현형의 학습은 더 간단한 표현형들의 조합을 통해 만들어집니다. 다른 말로, 추출된 특징들로 또 특징들을 추출해 내는 과정이 계속됩니다. 만약 feed-forward(순방향)으로 한 방향으로만 특징이 전달된다면 당연히 깊은 구조일수록 각 특징들 사이의 조합이 잘 이루어질 것입니다.

![](http://i.imgur.com/WMwglVE.png)
![](http://i.imgur.com/keotRRI.png)

## Curse of Dimensionality

머신러닝에서 차원의 저주는 입력 데이터의 차원이 커지면 커질수록 기하급수적으로 훈련이 힘들어지는 것을 의미합니다. N-dim line/square/cube... 에서 각 점이 가장 가까운 꼭지점을 찾는다고 생각할 때, 모든 점은 1,2,3,4... 차원에서 2,4,8,16...개 가능성을 가지게 됩니다. 즉, 점점 입력 데이터만으로 가능성을 모두 분류하는 것이 힘들어집니다. 더 복잡한 문제일수록 더 빠르게 차원의 저주에 직면하게 됩니다. 
- 만약 준비한 데이터의 양보다 가능성의 개수가 훨씬 많아진다면? 
- 그런데 우리가 필요한 결과는 딱 10종류밖에 없다면?
- 거기다 어느 점으로 뭉쳐야 하는지도 모른다면? 애초에, 뭉칠 수 있는지도 모른다면?

필연적으로 딥러닝에서는 통계적 접근이 필요하게 됩니다. 

![](http://i.imgur.com/S6YBV0M.png)

Locally(지역적으로) 볼 때는 여러 점들이 같은 결과를 낼 것이라고 생각하는 방법밖에 없습니다. 그렇다면, 엄청 높은 차원의 공간임에도 불구하고 어느 정도는 같은 점들끼리 뭉쳐 있는 '구역'을 가정한 것이 됩니다. 이제 locality를 가정한 채 확률적으로 이 점(즉, 데이터)이 이 '구역'에 속할지 말 지를 고민하는 문제가 됩니다. 이 문제는 구역의 경계만 구하면 / 경계 주변에서 두 인접한 구역에 속할 확률만 알면 간단히 해결될 수 있습니다!

## Manifold Learning

차원의 저주를 해결하기 위해 구역을 뭉치는 것 말고도 다른 방법이 있습니다. 몇몇 차원을 무시하면 됩니다. 우리가 원하는 결과가 3개밖에 없는데 가능성이 10가지라면, 첫 번째 결과는 1, 4, 6, 7번 차원만을 사용하고, 두 번째 결과는 2, 3번 차원만을 이용하고, 세 번째 결과는 5, 8번 차원만을 이용하게 할 수 있습니다. Manifold는 이처럼 차원을 '접어' 보는 접근입니다. 물론 어떤 차원을 접어야 제일 좋은지는 아무도 모릅니다.

여기서 좀 덜 극단적인 방법을 택하면, 모든 차원을 사용하되 weight(가중치)를 두어 어느 차원은 많이 중요하고 어느 차원은 적게 사용하는 방법이 있습니다. 

## Perceptron

딥러닝의 가장 기본이 되는 레고 블록은 perceptron(퍼셉트론)입니다. 무려 1958년부터 있던 개념입니다. 퍼셉트론은 인간 뇌의 neuron(뉴런)과 비슷한 역할을 합니다. 입력(들)을 받아 출력(들)을 만들기 때문입니다. 딥러닝에서는 거의 여러 개의 입력을 받아 한 개의 출력을 내는 퍼셉트론을 사용합니다. 대신 퍼셉트론을 아주 많이 사용합니다. 잘 생각해 보면 여러 개짜리 퍼셉트론은 한 개짜리 퍼셉트론을 여러개 붙여 놓은 것과 거의 같습니다.

![](http://i.imgur.com/XaAVbgx.png)
![](http://i.imgur.com/qVxgVlx.png)

위에서 이야기한, 간단한 개념에서 큰 개념으로의 조합, 차원의 저주를 해결하기 위한 구역의 경계 결정과 차원의 중요도를 결정하기 위한 가중치 문제는 퍼셉트론 개념에 모두 녹아 있습니다.

![](http://i.imgur.com/chp5dza.png)

퍼셉트론은 이전 층의 정보를 사용해 새로운 층의 정보를 만듭니다. 정보를 조합하기 위해 가중치를 두어 모두 더하고, bias(편향치)를 두어 새로 만들어진 정보를 보정합니다. 마지막으로 Activation function(활성화 함수)를 통과시키면 새로운 층의 새로운 차원에서 어느 정도로 어느 구역에 걸쳐 있는지를 결정할 수 있습니다.

#### Vector View

벡터 차원에서 보면 퍼셉트론은 입력 벡터가 가중치 벡터와의 내적을 통해 얻은 값을 결과로 내보냅니다. 즉, 가중치 벡터와 입력 벡터가 같은 방향을 가리킬 수록 내보내는 값은 커지게 됩니다. 벡터는 한 방향을 가리키는 특징이 있으므로, 퍼셉트론은 입력 벡터의 한쪽 방향으로의 성분을 의미합니다. 결과값은 대략적으로 구역을 간단히 가중치 벡터의 방향인지(즉, 내적해서 0보다 큰 구역) 아닌지(내적해서 0보다 작은 구역)로 나눕니다.

#### Non Linearity

대부분 활성화 함수로는 비선형 함수를 사용합니다. 결과를 그대로 내보내는 선형 함수의 경우 경계를 갈라 줄 수가 없기 때문이기도 하고, 선형 함수를 사용하는 경우 (bias를 무시하면) 두 층을 나눠 놓아도 선형대수학을 통해 한 층으로 합쳐 버릴 수 있기 때문입니다.

#### Matrix View

퍼셉트론은 보통 그 자체로 보지 않고 행렬로 묶어서 봅니다. 즉, n차 입력 벡터를 m차 입력 벡터로 바꿔주는 n x m matrix를 생각합니다.

![](http://i.imgur.com/pSwesoz.png)

장점은, 여러 개의 데이터가 동시에 들어와도 동시에 계산할 수 있다는 점입니다. CPU/GPU 모두 속도 측면에서 하나씩 하는 것보다 한번에 하는 게 분명한 향상이 있기 때문에, 보통 아래 방법을 사용합니다. 가중치를 저장할 때도 행렬 형태로 만들고 저장합니다.


## Types of Neural Networks

NN은 크게 feedback(피드백)이 있는 NN과 그렇지 않은 NN으로 나누어집니다. 피드백이 없는 NN에는 
- MLP(Multi Layer Perceptron)
- CNN(Convolutional Neural Network)

등이 있고 피드백이 있는 NN으로는
- RNN(Recurrent Neural Network)

등이 있습니다. 더 자세히 들어가면 세부적인 분류가 엄청 다양하고, 최근에는 두 가지 이상을 섞어 사용하거나 Attention(주목) 모델, Memory(메모리) 모델, RL(Reinforcement Learning)과의 결합 등 다양한 형태의 새로운 NN이 나오고 있습니다.

## Where to go?

딥러닝은 특유의 깊은 구조 때문에 훈련도 힘들고 데이터도 많이 필요하지만 점점 제약을 극복해 나가고 있습니다. 구글이나 바이두 같은 회사들은 어마어마한 데이터와 GPU로 최고의 성능을 뽑아내고 있고, 점점 더 깊은 구조가 나오고 있습니다. 그렇게 한쪽에서는 더 깊게, 더 뛰어나게 만들려고 하고 있습니다. 한편, 한쪽에서는 너무 깊은 구조를 압축해서 더 간단하게, 하지만 성능은 유지하게 만들려고 하고 있습니다. 또 한편에서는 깊게, 하지만 가볍게 만들려고 하고 있습니다. 그리고 한편에서는, 왜 딥러닝이 이렇게 잘 동작하는지 알아내려고 하고 있습니다.

이 분야에서 연구해 볼 것은 너무 많고 알아야 할 것도 너무 많은데, 엄청 빠르게 변하기까지 하는 분야입니다. 화이팅!

딥러닝의 어마어마한 표현 공간을 모두 사용할 수 있다는 점 때문에, 적절한 훈련이 이루어지면 딥러닝은 보통 과적합이 됩니다. 이 경우 분산만 조금 끌어내리면 아주 좋은 결과를 얻을 수 있습니다. 문제는 Bias-Variance 사이에는 보통 Tradeoff(교환)이 있기 때문에, 얼마나 끌어내려야 하는지, 어느 정도까지 끌어내려야 하는지는 아직까지는 heuristic(시행착오를 통한 지식)의 영역입니다.




## Regularization

Regularization(정규화)는 머신러닝에서 훈련 데이터로 훈련시킨 모델을 훈련 데이터뿐만이 아니라 다른 데이터에도 잘 들어맞을 수 있도록 해 주는 것입니다. 주로 학습 방법을 변경해 정규화를 달성하려고 합니다. 훈련 데이터 외의 데이터(테스트 데이터)에서 발생하는 에러를 generalization error라고 한다면, 정규화는 generalization error를 줄이는 것을 목표로 하지, training error를 줄이는 것을 목표로 하지는 않습니다.

보통 정규화는 bias-variance tradeoff(편향-분산 교환)에서 variance를 줄이는 방향으로 적용합니다. 즉, bias를 늘릴 수도 있습니다. 좋은 정규화 기법일수록 편향은 많이 키우지 않으면서도 분산은 많이 줄여 과적합을 벗어나려고 합니다. 여기서는 거의 무한한 함수 표현 능력을 가진 NN에 constraint(제한)을 걸어 과적합되지 않도록 하는 방법을 다룹니다.

이 분야에서 연구해 볼 것은 너무 많고 알아야 할 것도 너무 많은데, 엄청 빠르게 변하기까지 하는 분야입니다. 화이팅!