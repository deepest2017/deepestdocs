## Importance of Initialization

딥러닝에서는 다양한 변수들을 이용해 네트워크를 구성합니다. 변수들은 훈련을 통해 계속 바뀌지만, 제일 처음 어떤 값으로 초기화(initialization) 되었느냐에 따라 성능은 크게 달라집니다. 초기화가 필요한 변수로는 가중치(weight)와 편향치(bias)가 있습니다. 어떻게 가중치와 편향치를 잡는게 최적의 성능을 내는지는 아직도 밝혀지지 않았습니다. 응용에 따라 서로 다른 방법에서 최고의 성능이 나오기도 합니다.

초기화에 대해 유일하게 알고 있는 사실은, 대칭성(symmetry)을 깨는 방향으로 초기화를 해야 한다는 것입니다. 같은 입력에 대해 동일하거나 대칭적인 가중치로 초기화를 하면 각 차원의 표현력이 최대한으로 발휘되지 못하고, 이는 표현할 수 있는 범위가 줄어드는 결과로 이어집니다. 서로 다른 가중치는 서로 다른 벡터를 만들고, 서로 다른 특징에 집중할 수 있도록 해 줍니다.

일반적으로 가중치는 무작위 균일(uniform) 혹은 가우시안(Gaussian) 확률분포에서 추출한 값을 잡고 편향치와 그 외 값들은 미리 정해놓은 상수값(constant)들로 잡습니다. 초기화를 잘 못하면 아주 이상한 결과를 얻거나 형편없는 결과를 얻을 수도 있습니다! 그래서 무작위도 요령있게 잘 뽑으려는 시도가 많이 생겼습니다. 그 요령은 보통 가중치의 분산(variance)을 잘 정하는 것입니다. 가중치의 평균(mean)은 0으로 잡습니다.

분산의 초기값이 클수록 대칭성을 더 잘 깨 줄 것이라고 생각할 수 있지만, 한편으로는 값들이 너무 커져 정방향 전파와 역방향 전파 모두에서 폭발(explode)할 수도 있습니다. 특히 RNN에서는 큰 문제가 됩니다. 값이 너무 크거나 너무 작으면 몇몇 활성 함수(activation function)들에서는 기울기가 거의 0이 되어, 훈련이 안 되기도 합니다. 한편, 분산의 초기값이 작으면 상대적으로 더 작은 범위만을 보게 되고 가중치 벡터들 간 상관성이 크게 증가할 것입니다. 이처럼 값이 크면 좋다는 것과 작으면 좋다는 것 사이에서 균형을 잡아야 합니다.

다른 관점도 있습니다. 분산의 초기값이 클수록 다양한 표현 공간(representation space)를 더 많이 탐색할 수 있을 것이라고 예측할 수 있습니다. 하지만 우리에게 필요한 표현 공간에 집중하기 위해서는 더 많은 최적화 과정을 거쳐야 하겠지요. 일반적으로 딥러닝에서 표현 공간이 부족한 일은 없기 때문에, 가중치를 어느 정도 제한해도 좋은 성능을 얻을 수 있습니다.

---

## Weight Initialization

균일 분포의 경우 low, high 값을 잡아주면 확률분포를 생성할 수 있습니다. 가우시안 분포의 경우 mean, std 값을 잡아주면 확률분포를 만들 수 있습니다. 아래에서 실험적으로 잘 동작하는 초기화 방법을 이야기하겠지만, 굳이 저 방법을 따를 필요는 없습니다. 요즘은 그냥 \\(N(0, 0.01^2)\\) 정도의 작은 값으로 일괄적으로 초기화하기도 합니다. 

만약 모든 입력 \\(X\\)가 가중치 \\(W\\)와 곱해져 \\(Y\\)를 내는 선형 시스템으로 인공신경망을 생각한다면, \\(X\\)의 분산에 따른 \\(Y\\)의 분산은 다음과 같이 계산할 수 있습니다. 여기서는 \\(X\\)와 \\(W\\)의 평균이 0이고, 각 입력에 대해 모두 iid 라는 가정을 사용했습니다.

$$ Y = W_1 X_1 + W_2 X_2 + ... W_n X_n $$
$$ Var(Y) = \sum_i Var(W_i X_i) = \sum_i Var(W_i)Var(X_i) \\
= nVar(W)Var(X) $$

\\(Y\\)의 분산은 \\(n\\)이 커질수록 엄청나게 커지게 됩니다.위 계산에서 어차피 \\(X\\)의 분산은 정해져 들어오는 것이므로, \\(W\\)의 분산을 줄이는 것이 결국 \\(Y\\)의 분산을 줄이는 길이 됩니다. 여기서 \\(n\\)은 들어오는 입력의 수를 의미합니다. 그런데, 우리가 줄이고 싶은 분산은 정방향으로 계산할 때만이 아니라 역방향으로 계산할 때에도 있으므로 입력의 갯수 \\(n_{in}\\)과 출력의 갯수 \\(n_{out}\\)가 필요합니다. 

MLP에서  \\(n_{in}\\)과 \\(n_{out}\\)는 다음과 같이 계산합니다.

* \\(n_{in}\\) 입력 벡터의 차원 수
* \\(n_{out}\\) 출력 벡터의 차원 수

CNN에서  \\(n_{in}\\)과 \\(n_{out}\\)는 다음과 같이 계산합니다. CNN에서 픽셀 수는 MLP에서의 차원 수와 유사합니다. 맵 갯수를 곱하는 항은 각 가중치가 그만큼 더 여러 맵에서 사용되고 더해지기 때문입니다.
*  \\(n_{in}\\) 입력 맵의 갯수 * 각 맵의 픽셀 수
* \\(n_{out}\\) 출력 맵의 갯수 * 각 맵의 픽셀 수

**LeCun initialization**

아직 ReLU가 세상에 알려지지 않았을 때부터 인공신경망을 연구하던 LeCun 이 제안한 방법입니다. Caffe 에서도 기본값은 이런 방식으로 만들어져 있다고 합니다. 아래 식을 보면, 가우시안 분포에서는 분산을 \\(X\\)의 원래 분산 정도로 보정해 주는 것을 알 수 있습니다. 
$$ Var(W) = \frac{1}{n_{in}} $$

균일 분포에서 왜 하필 3을 사용하는지는 LeCun 이 시행착오를 거쳐 찾아냈을 것이라고 짐작합니다.
$$ High(W) = \sqrt{\frac{3}{n_{in}}},  Low(W) = -\sqrt{\frac{3}{n_{in}}} $$

이 방법은 실제로 sigmoid 활성함수를 쓸 때 잘 통했습니다. Sigmoid 함수는 0 근처에서는 선형 함수에 가깝고, 위아래 값이 정해져 있기 때문에 분산이 엄청나게 튀지 않습니다.

**Glorot initialization**

한편, 위 LeCun 방식은 ReLU가 등장하고, 또 엄청 깊고 넓은 인공신경망이 등장하며 좋은 성능을 내지 못했습니다. Glorot이 2010년 제안한[1] 초기화 방법은 다음과 같습니다. Xavier 초기화라고 하기도 하고 Glorot 초기화라고 하기도 합니다. 특별한 응용 분야가 아닌 이상, 보통 Glorot 초기화를 합니다. 확실한 성능 향상을 보여준 초기화 방법이었습니다. 정방향과 역방향 모두에서 분산을 조절할 수 있게 됩니다.
$$ Var(W) = \frac{2}{n_{in} + n_{out}} $$

균일 분포에서도 LeCun 방식과 유사합니다.
$$ High(W) = \sqrt{\frac{6}{n_{in} + n_{out}}}, Low(W) = -\sqrt{\frac{6}{n_{in} + n_{out}}} $$

> [1] Understanding the Difficulty of Training Deep Feedforward Neural Networks
> (Xavier Glorot, Yoshua Bengio, 2010)

**He initialization**

He 방식[2]은 위 Glorot 방식과 유사하지만 \\(n_{out}\\)을 고려하지 않습니다. ReLU가 음수 쪽 신호를 완전히 없애버린다는 것에서 착안해, 분산을 두배 해 줘야 분산을 유지할 수 있다는 생각입니다.
$$ Var(W) = \frac{2}{n_{in}} $$

> [2] Delving Deep into Rectifiers: Surpassing Human-Level Performance of ImageNet Classification
> (Kaiming He, Xiangyu Zhang, Shaoquing Ren, Jian Sun, 2015)

**Orthogonal initialization**

특이값 분해(singular  value decomposition, SVD)를 통해 가중치 행렬의 각 행(또는 열)이 모두 수직이 되도록 만드는 초기화 방법[3]도 있습니다. 이 경우 각각의 벡터가 정말로 처음부터 다른 특성에 집중하게 되길 기대합니다. 다만 이 초기화 방법의 경우 가중치 행렬에 어떤 값 \\(g\\)를 곱해 줘서 크기를 보정해 줘야 최적의 성능을 얻을 수 있다고 합니다.

> [3] All You Need is a Good Init
> (Dmytro Mishkin, Jiri Matas, 2016)

---

## Bias Initialization

보통 편향치는 전부 0이나 0.01같이 아주 작은 양수를 사용합니다. 초기에는 ReLU 때문에 시작부터 동작하지 않는 죽은 뉴런(dead neuron)들을 방지하기 위해 편향치로 0.01, 0.1 정도의 값을 사용했었지만, 훈련 기법이 발전한 지금은 거의 언제나 0으로 초기화합니다. 편향치는 가중치보다 수는 훨씬 적지만 활성화 함수에 직접적으로 연결되기 때문에 엄청 중요합니다. 

만약 데이터에 한 종류의 클래스만 엄청 편향되어 있다면, 마지막 softmax 층의 편향치는 고르게 들어오는 입력값이 softmax 후 훈련 데이터의 비율과 비슷해지도록 잡아 주는 것이 좋다고 합니다. 대부분의 분류 문제에서 보통 클래스 당 데이터는 비슷한 정도로 들어 있으니 일괄적으로 0으로 잡아줘도 괜찮습니다.

---