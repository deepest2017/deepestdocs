## Activation Function

딥러닝은 여러 층(layer)을 쌓아 데이터에 대한 표현을 학습하는 과정입니다. 각 층에서는 일반적으로 가중치(weight)를 곱하고 편향치(bias)를 더하는데, 이 과정은 완전히 선형(linear)입니다. 선형 결합의 특성상, 아무리 많은 층을 쌓아도 선형 결합끼리 선형 결합하면 다시 선형 결합밖에 되지 않습니다.

우리가 하고자 하는 것은 딥러닝을 통해 각 데이터를 원하는 점(들)로 매칭하는 것입니다. 이 점(들)은 확률 분포일 수도 있고, 특정 값일 수도 있고, 신뢰도일 수도 있습니다. 이 과정은 대단히 복잡한 함수를 통해 이루어질 것입니다. 하지만 세상의 많은 데이터들은 우리가 원하는 특징을 칼같이 선으로 나눌 수 있게 갖고 있지 않습니다. 이렇게 엉켜 있는 표현형(entangled representation)을 선형 함수로는 구분하고 재배열할 수 없기 때문에, 비선형(nonlinear) 함수가 필요합니다. 딥러닝에서는 매 층이 끝날 때마다 비선형 함수를 적용하고 있습니다.

* 각 층에서 계산한 결과에 적용하는 함수를 활성 함수(**activation function**)이라고 합니다.
* 활성 함수를 지나고 나온 각 층의 결과물을  활성값(**activation**)이라고 합니다.
* Softmax 활성 함수에 들어가는 값은 로짓(logit)이라고 하기도 합니다.

---

## Non-parametric Activations

활성 함수는 보통 훈련 중에 바뀌지 않습니다.대표적으로 사용하는 활성함수들은 다음과 같습니다. 모두 _tf.nn_ 안에 포함되어 있습니다.

**Sigmoid**
0부터 1 사이의 값을 갖기 때문에 보통 스케일링한 픽셀 값을 나타내거나 이진 분포에 대한 확률로 사용합니다. 한때는 모든 층에 sigmoid를 사용했었지만 이제는 잘 사용하지 않습니다. 미분한 형태가 좋아 많이 사용했었습니다. 같은 크기의 다른 벡터에 원소별로(element-wise) 곱을 해 몇 %를 살릴 지 결정하는 마스크(mask)처럼 사용하기도 합니다. 이 경우 마스크라는 표현 대신 게이트(**gate**)라고 하는 편입니다.
$$ y = \sigma(x) = \frac{1}{1 + e^{-x}} $$

**Softplus**
부드러운 상승곡선을 그리는 활성 합수입니다. 미분하면 sigmoid가 됩니다. 만약 sigmoid를 취한 값에 다시 로그를 취할 일이 있다면, 한 번에 softplus를 적용합니다.
$$ y = \log(1+e^{x}) = -\log(\sigma(-x))$$

**Tanh**
말이 필요없는 tangent hyperbolic 함수. -1부터 1 사이의 값을 갖습니다. 스케일링한 픽셀 값을 나타내기도 합니다. Sigmoid보다 더 표현 가능한 범위가 넓고 기울기 특성도 좋아 상하한(bound)가 필요하지만 표현력도 필요한 재귀형 신경망(recurrent neural network)에서 많이 보입니다.
$$ y = \tanh(x) $$

**ReLU**
꺾인 선형(rectified linear unit)의 줄임말입니다. 값이 음수이면 다음 층부터는 영향을 전혀 주지 않게 만드는 활성 함수입니다. 조건문이기 때문에 역전파 알고리즘을 위해서는 정방향 전파 때 어떤 경로로 전달되었는지를 기록해야 합니다. 값이 음수이면 언제나 기울기로 0을 받습니다. 한편, 값이 양수이면 얼마나 값이 크던지 늘 기울기로 1을 받게 됩니다. 앞의 함수들이 중첩(합성)할 경우 계속 값이 작아지는 것과 다르게, ReLU 에서는 여러 번 합성한다고 값이 작아지지 않기 때문에 깊게 층을 쌓는 데 아주 좋습니다. 대신 활성값들이 커지는 데 제한이 없기 때문에 보통 몇 번 층을 쌓고 나면 활성값들이 엄청 커져 있는 것을 볼 수 있습니다.
$$ y = \max(x,0)  = (x >0) ? x : 0$$

값이 음수라서 기울기가 계속 0을 받는 경우가 생길 수 있습니다. 이 경우 죽었다고(dead neuron, dead perceptron) 표현하기도 합니다. 기울기가 없는 구간이 있기 때문에 듬성듬성한(sparse) 활성 함수의 한 종류입니다.

**Leaky ReLU**
ReLU 패밀리에 들어갑니다. 값이 음수인 구간에서 아주 작지만 기울기가 있습니다. 따라서 듬성듬성하지 않습니다. 보통 \\(\alpha\\)로는 0.02, 0.1 정도를 사용합니다.
$$ y = (x > 0) ? x : \alpha x $$

**Softmax**
Softmax는 값들을 합이 1이고 늘 양수인 확률분포로 표준화(normalize)하는 함수입니다. Softmax를 지나도 값의 상대적인 순위는 변하지 않습니다.
$$ y_k = \frac{e^{x_k}}{\sum_i e^{x_i}} $$

**Linear**
엄밀히 말해 비선형 함수는 아니지만, 가끔 다음 층으로 활성 함수를 거치지 않은 값을 보내고 싶을 때 사용합니다. 사용하지 않는 것과 동일합니다. 가끔은 편향치만 더하고 싶을 때도 있습니다.
$$ y = x \\
y = x + b $$

**Softsign**
+1 혹은 -1을 만드는 부호 함수는 미분 불가능한 함수라, 역전파 알고리즘에 적합하지 않습니다. 그래서 +1 혹은 -1을 만들면서도 미분 가능하도록 부드럽게(soft) 변형한 함수입니다.
$$ y = \frac{x}{(|x|+1)} $$

**ELU**
지수 선형 함수(exponential linear unit)의 약자입니다. ReLU를 통과한 후의 활성값의 평균은 언제나 0보다 양수 쪽으로 편향되어 있을 수 밖에 없습니다. ELU를 통과한 값들의 평균이 ReLU 보다 좀 더 0에 가깝고, 음수 부분에서도 기울기가 생겨 학습이 잘 된다고 합니다. 잘 안 씁니다.
$$ y = (x > 0) ? x : e^x - 1 $$

---

## Parametric Activations

학습 가능한 변수를 활성 함수에 추가해 더 뛰어난 성능을 얻으려 하는 시도들이 여럿 있었습니다. 이 경우 대부분 ReLU 의 기울기를 훈련시킵니다. PReLU를 시작으로 SReLU, RReLU, CReLU 등이 있습니다.

**Parameteric ReLU**

변수가 있는 ReLU(parametric ReLU, PReLU)는 leaky ReLU의 \\(\alpha\\) 값을 학습하도록 합니다. ReLU 및 leaky ReLU를 포함하고 있습니다.
$$ y_i = (x_i > 0) ? x_i : \alpha_i x_i $$

---

## Various Activations

잘 쓰이진 않지만 활성 함수로 가끔 보이는 함수들입니다.

**Softmax distillation**

기존의 softmax와 동일하지만, 값들을 우선 온도(temperature) \\(T\\)로 나누고 softmax를 취하는 함수입니다. 이 경우 값은 기존보다 훨씬 부드럽게 분포하는 값이 됩니다. 일반적인 softmax는 \\(T = 1\\)인 경우라고 생각할 수 있습니다.

[-3, 2, 0]은 \\(T = 1\\)에서 [0.006, 0.876, 0.118]이 됩니다.
\\(T = 2\\)에서는 [0.057, 0.690, 0.253] 이 되고, \\(T = 10\\)에서는 [0.250, 0.412, 0.338]이 됩니다. 온도를 높일수록 격차가 줄어듭니다. 즉, 정답이 아니더라도 다른 클래스들과의 상대적인 관계 정보가 더 포함된다고 해석할 수 있습니다.

**Thresholded ReLU**
기본적으로 ReLU와 동일하지만, 추가로 0부터 \\(\theta\\)까지의 값도 0입니다.
$$ y = (x > \theta) ? x : 0 $$

**Clipped ReLU, ReLU6**
너무 값이 커지는 것을 방지하기 위해 ReLU의 위쪽에 상한선을 둡니다. 보통 상한선 값으로 6을 많이 쓰기 때문에 이 경우 ReLU6 이라고 하는 것 같습니다.
$$ y = (x < 6) ? ((x > 0) ? x : 0) : 6 $$

---